\documentclass{article}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{color}
\usepackage{xcolor}
\usepackage{amssymb}
\usepackage{mathtools}
\definecolor{keywordcolor}{rgb}{0.8,0.1,0.5}
\usepackage{listings}
\usepackage{xcolor}
\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}
\lstset{ %
backgroundcolor=\color{white},      % choose the background color
basicstyle=\footnotesize\ttfamily,  % size of fonts used for the code
columns=fullflexible,
tabsize=4,
breaklines=true,               % automatic line breaking only at whitespace
captionpos=b,                  % sets the caption-position to bottom
commentstyle=\color{mygreen},  % comment style
escapeinside={\%*}{*)},        % if you want to add LaTeX within your code
keywordstyle=\color{blue},     % keyword style
stringstyle=\color{mymauve}\ttfamily,  % string literal style
frame=single,
rulesepcolor=\color{red!20!green!20!blue!20},
% identifierstyle=\color{red},
language=c++,
}
\lstset{breaklines}
\lstset{extendedchars=false}

\author{Bao Jinge}
\title{Solutions for Week 4}
\date{}

\begin{document}
	\maketitle
	\section{}
	As pigeonhole principle tells. there must be two same element in $\{x_1, x_2, x_3\}$, like two 1s, or two 0s.
	Thus we can design a Las Vegas algorithm like this:\\
	1.We ramdomly choose two bits of $\{x_1,x_2,x_3\}$to read and calculate the sum of them.
	2.If sum of these two bits are 2 or 0, then we got $f(x_1,x_2,x_3)$.
		$$f(x_1,x_2,x_3)=
		\begin{cases}
		1& \text{sum of two bits is 2}\\
		2& \text{sum of tow bits is 0}\\
		\end{cases}$$
	3.When sum of these two bits is 1, we scan the last bit and calculate the sum of all. Then $f(x_1,x_2,x_3)$ is detemined as definition.
	We can prove it easily. Let $X$ denotes the bits need to scan.
	$$E(X)=Pr(X=2) \cdot 2 + Pr(X=3) \cdot 3
			=\binom{3}{2} \cdot 2 + (1-\binom{3}{2}) \cdot 3
			=\frac{8}{3}$$
	Yao's Minimax Principle needs to find a worst input distribution on a determistic algorithm. Because of this algorithm is for specific distribution not a randomized distribution, we can't find a more worst distribution for this problem. Thus, we can't use Yao's Minimax Principle to improve it.

	\section{}
	Suppose P is mixed strategy distribution of Row and Q is mixed strategy distribution of Col.
	$p_i \in P=\{p_1, p_2\}$ and $q_j \in Q=\{q_1, q_2\}$. $p_i$ denotes probability to choose action i and so do $q_j$.
	Let $V(P,Q)$ donates the expected payoff of dollars Row paying Col, we got
	$$V(P,Q) = \sum_{i=1}^{2}\sum_{j=1}^{2}p_iq_jC(i,j)$$
	What Row to do is 
	$$\max_{P}\min_{Q}V(P,Q)$$
	And what Col to do is
	$$\min_{Q}\max_{P}V(P,Q)$$
	As Von Neumann's Minimax Theorem,
	$$\max_{P}\min_{Q}V(P,Q)]\leq V^{*} \leq \min_{Q}\max_{P}V(P,Q)$$
	\begin{equation}
	\begin{aligned}
	V(P,Q)&=\frac{15}{4}p_1q_1-\frac{9}{4}p_1-\frac{5}{2}q_1+\frac{3}{2}\\
	&=\frac{15}{4}(p_1-\frac{2}{3})(q_1-\frac{3}{5})
	\end{aligned}
	\end{equation}
	Thus the equilibrium point of this zero-sum game is $p_1=\frac{2}{3}$ and $q_1=\frac{3}{5}$.
	We can simplely demonstrate that when $p_1 \neq \frac{2}{3}$, $q_1$ could always choose some value make $V$ less than $0$. And when $q_1 \neq \frac{3}{5}$, $p_1$ could always choose some value make $V$ greater than $0$
	Let $X$ denotes action which Row choose and $Y$ denotes action which Col choose
	So the minimax optimal strategies for Row is 
	$$Pr(X=1)=p_1=\frac{2}{3}$$
	$$Pr(X=2)=p_2=\frac{1}{3}$$
	$$Pr(Y=1)=q_1=\frac{3}{5}$$
	$$Pr(Y=2)=q_2=\frac{2}{5}$$
	And the expected payoff $V^{*}$ is $0$.
	    
	\section{}
	Each processor will get $\frac{n}{m}$ jobs. Let $T_{ij}$ denotes j-th job on i-th processor completed in 1 step or k steps. Let $T_{i}$ donotes the running steps of i-th processor.
	$$T_{i}=\sum_{j=1}^{\frac{n}{m}}T_{ij}$$
	$$
	T_{ij} = 
	\begin{cases} 
	1& \text{w.p. p} \\
	k& \text{otherwise}\\
	\end{cases}
	$$
	$$\mathbb{E}(T_{ij})=p+k(1-p)$$
	Suppose random variable $X_{ij} = \frac{T_{ij}-k}{1-k}$
	$$
	X_{ij}=
	\begin{cases}
	1& \text{w.p. p} \\
	0& \text{otherwise}\\
	\end{cases}
	$$
	$$X_{i}=\sum_{j=1}^{\frac{n}{m}}X_{ij}$$
	$$\mathbb{E}(\sum_{j=1}^{\frac{n}{m}}X_{ij})=\sum_{j=1}^{\frac{n}{m}}\mathbb{E}(X_{ij})=\frac{n}{m}p=\mu$$
	As for$$X_{ij}=\frac{T_{ij}-k}{1-k}$$
	We could get
	$$T_{i}=(1-k)X_{i}+k\frac{n}{m}$$
	Here we use Chernoff bound
	$$
	\begin{aligned}
	Pr(\left| X_{i}-\mu\right|\geq \delta\mu) &\leq 2e^{-\frac{\mu\delta^2}{3}}\\
	&\leq 2e^{-\frac{np\delta^2}{3m}}
	\end{aligned}
	$$
	When we choose 
	$$\delta = \delta^{*} = \sqrt{\frac{3mc\ln{(n)}}{np}}$$, which $c>1$
	we get
	$$
	Pr(\left| X_{i}-\mu\right|\geq \delta\mu) \leq \frac{2}{n^c}
	$$
	And with relations between $T_{ij}$ and $X_{ij}$
	$$
	\begin{aligned}
	\left| X_{i}-\mu\right| &\leq \delta^{*}\mu\\
	\left| \frac{T_{i}-\frac{n}{m}k}{1-k} - \mu\right| &\leq \delta^{*}\mu \\
	\frac{n}{m}(k-kp+p)+(1-k)\sqrt{\frac{3cnp\ln(n)}{m}} &\leq T_{i} \leq \frac{n}{m}(k-kp+p)-(1-k)\sqrt{\frac{3cnp\ln(n)}{m}}
	\end{aligned}
	$$
	So we got upper bound and lower bound on when all jobs will be completed as above w.h.p., which $c>1$ and $n\geq m$.

\end{document}














